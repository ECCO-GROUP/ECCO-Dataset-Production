# aws_config.yaml
# =================================================================================================
# Contains values that are specific to AWS processing (AWS S3/AWS Lambda)
# This includes AWS paths and names, job control values, and Lambda function values
# SOME VALUES ARE SPECIFIC TO EACH VERSION, SO ENSURE VALUES ARE ACCURATE FOR ECCO VERSION


# String
# AWS Account ID
# AWS Account (role): ecco-production-aws (448078824696)
# https://sso3.jpl.nasa.gov/awsconsole
account_id: '448078824696'


# =================================================================================================
# CREDENTIALS VALUES (https://github.jpl.nasa.gov/cloud/Access-Key-Generation)
# =================================================================================================
# String
# Name of AWS Credential profile to use for authentication
profile_name: 'saml-pub'

# String
# AWS region to use
region: 'us-west-2'

# String
# Method to use for updating credentials (either 'binary', or 'python')
credential_method_type: 'python'

# String
# Path to AWS credential updating file
# Default: ECCO-Dataset-Production/processing/src/utils/aws_login/update_AWS_cred_ecco_production.sh
aws_credentials_bash_filepath: ''


# =================================================================================================
# AWS S3 Information
# =================================================================================================
# String
# Name of S3 bucket to source ECCO model output files
source_bucket: 'ecco-model-granules'

# String
# Subfolder of source_bucket to source ECCO model output files
bucket_subfolder: 'V4r4'

# String
# Derived products bucket to source/save any vector rotated products. Saves files to the subfolder {ecco_version}
derived_bucket: 'ecco-derived-products'

# String
# Name of S3 bucket to save processed datasets. Saves files to the subfolder {ecco_version}
output_bucket: 'ecco-processed-data'

# String
# String path to upload files from local to AWS S3
# Default: {source_bucket}/{bucket_subfolder}
upload_to_S3_path: ''


# =================================================================================================
# AWS LAMBDA Information
# =================================================================================================
# String
# URI of Docker image to use from AWS Elastic Container Registry (ECR)
image_uri: '448078824696.dkr.ecr.us-west-2.amazonaws.com/ecco_processing:latest'

# String
# Name of the role for the lambda's to assume when executing
role: 'lambda-role'

# String
# Lambda function name prefix (2D/3D_native/latlon appended)
function_name_prefix: 'ecco_processing'


# =================================================================================================
# JOB SETUP AND CONTROL (EXECUTION TIMES, MAX EXECS, TIMEOUT, and RETRY)
# =================================================================================================
# Boolean
# Controls whether or not to invoke each lambda job in parallel
use_workers_to_invoke: True

# Float
# Time to process single 1D vertical level (s)
1D_time: 0.5

# Float
# Time to process single 2D latlon vertical level (s)
latlon_2D_time: 0.5

# Float
# Time to process single 3D latlon vertical level (s)
latlon_3D_time: 0.075

# Float
# Time to process single 2D native vertical level (s)
native_2D_time: 0.8

# Float
# Time to process single 3D native vertical level (s)
native_3D_time: 0.05

# Int
# Override the max number of timestep executions to be processed in each lambda job.
# This value is used if lower than the calculated max_execs from time.
# This value will be ignored if set to 0
override_max_execs: 0

# Float
# Timeout (in seconds) for each job. Lambda has a timeout of 900s, this value
# will stop the job and print out necessary logging information, so it should be less than 900s.
job_timeout: 890.0

# Int
# Number of times to retry processing a failed timestep
num_retry: 1

# Int
# Number of batches to process.
# Number of batches is how many Lambda jobs are submitted for each grouping
# e.g. If there are 100 timesteps for a job, and each Lambda job is assigned 5 timesteps (max_execs)
# then there would be 20 Lambda jobs and 20 batches. If you set number_of_batches_to_process
# to 5, then only the first 5 Lambda jobs would be submitted, so only the first 25 time steps would
# be processed. 
# This helps to limit the number of Lambda jobs when there are a lot of jobs listed in the jobs.txt file.
# Default: -1 (does not limit the number of Lambda jobs submitted)
# A value of 0 means no batches will be processed.
number_of_batches_to_process: -1


# =================================================================================================
# LAMBDA FUNCTION MEMORY SIZES (1D, 2D/3D latlon, and 2D/3D native)
# =================================================================================================
# Int
# Memory size for 1D lambda jobs (MB)
memory_size_1D: 1024

# Int
# Memory size for 2D latlon lambda jobs (MB)
memory_size_2D_latlon: 1024

# Int
# Memory size for 3D latlon lambda jobs (MB)
memory_size_3D_latlon: 1536

# Int
# Memory size for 2D native lambda jobs (MB)
memory_size_2D_native: 1536

# Int
# Memory size for 3D native lambda jobs (MB)
memory_size_3D_native: 1536