# **/configs/**
Contains all the configuration files used to control and modify the processing and production of the ECCO datasets
- **aws_config.yaml**
  - Configuration file, containing values pertaining to AWS and its services.
    - account_id
    - profile_name
    - region
    - credential_method_type
    - aws_credentials_bash_filepath
    - source_bucket
    - bucket_subfolder
    - derived_bucket
    - output_bucket
    - upload_to_S3_path
    - image_uri
    - role
    - function_name_prefix
    - use_workers_to_invoke
    - 1D_time
    - latlon_2D_time
    - latlon_3D_time
    - native_2D_time
    - native_3D_time
    - override_max_execs
    - job_timeout
    - num_retry
    - memory_size_1D
    - memory_size_2D_latlon
    - memory_size_3D_latlon
    - memory_size_2D_native
    - memory_size_3D_native
    - number_of_batches_to_process
- **jobs.txt**
  - File containing list of jobs to execute. Jobs are defined as: gouping to process, product type, output frequency, number of time steps to process (e.g. [0,'latlon','AVG_MON','all'], or [10,'native','AVG_DAY',10]).
  - Optionally, one can have a line that is just "all", and all jobs will be calculated per the groupings metadata file, and executed.
  - A line that is just "done" will cause any jobs listed after to not be executed.
  - This file can be created automatically from user prompts when the user passes the "--create_jobs" argument to "master_script.py".
    - The user is prompted to select product types, frequencies, datasets, and timesteps that they would like to process. These selections are then modified in order to match the "job" format, and all of these jobs are saved the job file, "jobs.txt".
    - Note: Once created, this file should be modified at your own risk or just re-run "master_script.py" with the "--create_jobs" argument to make the file with your new choices.
- **product_generation_config.yaml**
  - Configuration file, containing values pertaining to the actual processing and production of ECCO datasets
    - create_checksum
    - download_all_fields
    - compare_checksums
    - use_workers_to_download
    - read_ecco_grid_for_native_load
    - extra_prints
    - binary_fill_value
    - array_precision
    - model_start_time
    - model_end_time
    - num_vertical_levels
    - geospatial_vertical_min
    - latlon_grid_resolution
    - latlon_effective_grid_radius
    - latlon_grid_area_extent
    - custom_grid_and_factors
    - grid_files_dir
    - custom_factors_dir
    - source_grid_min_L
    - source_grid_max_L
    - source_grid_ftype
    - target_grid_ftype
    - ecco_grid_dir
    - ecco_grid_dir_mds
    - mapping_factors_dir
    - ecco_code_name
    - ecco_code_dir
    - ecco_access_dir
    - ecco_configurations_name
    - ecco_configurations_subfolder
    - metadata_dir
    - model_output_dir
    - local_file_dir_to_upload
    - processed_output_dir_base
    - ecco_grid_filename
    - filename_tail_1D
    - filename_tail_latlon
    - filename_tail_native
    - podaac_metadata_filename
    - processing_code_filename
    - history
    - references
    - source
    - summary
    - doi
    - ecco_version
    - product_version
    - dataset_description_tail_1D
    - dataset_description_tail_latlon
    - dataset_description_tail_native
- **requirements.txt.**
  - Contains all the packages and version numbers for each required package to run any, and all, aspect of the dataset production code.
  - There may be extra/extraneous packages included, however all the necessary ones are included.
  - This file can be used to create a conda environment by using the following command, where "env" is the name of the environment to create:
    - conda create --name "env" --file requirements.txt